---
title: "Course Project: Classifying Weight Lifting Form"
subtitle: "Coursera - Practical Machine Learning"
date: September 20, 2014
output:
  html_document:
    keep_md: yes
---

### Synopsis
This report is a course project which uses the *Weight Lifting Exercise Dataset* from [Human Activity Recognition](http://groupware.les.inf.puc-rio.br/har) website. A random forest model is used to classify weight lifting forms from wearable accelerometers input.  The final model contains a total of 53 features with a 100% accuracy of prediction using the test dataset. 

### Data Details
The data was collected from six young health participants.  They were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification `Class A`, throwing the elbows to the front `Class B`, lifting the dumbbell only halfway `Class C`, lowering the dumbbell only halfway `Class D` and throwing the hips to the front `Class E`.

More information related to the data can be found used in the following research article:
Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. [Qualitative Activity Recognition of Weight Lifting Exercises](http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201). Proceedings of 4th 
International Conference in Cooperation with SIGCHI (Augmented Human '13) . 
Stuttgart, Germany: ACM SIGCHI, 2013.

### Data Processing
In order to load the data without the NA, !DIV/0, and empty string values, we treated the aforementioned three types of data as NA while loading the data.  

```{r setup, results='hide', cache=TRUE, comment=NA}
library(RCurl)
library(caret)
library(randomForest)
library(rpart.plot)
library(tree)

# read in training data
url1="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
x1 <- getURL(url1,ssl.verifypeer = FALSE)
training <- read.csv(textConnection(x1), header = TRUE
                     , na.strings = c("NA", "#DIV/0!", "")   
                     , quote ="\"")

# read in testing data
url2="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
x2 <- getURL(url2,ssl.verifypeer = FALSE)
testing <- read.csv(textConnection(x2), header = TRUE
                     , na.strings = c("NA", "#DIV/0!", "")   
                     , quote ="\"")

# record data access time
dateDowloaded<-date()
dput(dateDowloaded, "dateDownloaded.txt")
```

The training data is a 19622 by 160 data set and the testing data is a 20 by 160 data set.

```{r datadim, cache=TRUE}
dim(training)
dim(testing)
```

We divide the training set into `subtrain` and `subtest` data set with a 70/30 split for fitting and testing the model.

```{r makeTraining, cache=TRUE}
inTrain <- createDataPartition(y=training$classe, p=0.7, list=FALSE)
subtrain <- training[inTrain,]
subtest <- training[-inTrain, ]
```

After using `nafunc` to check how many of the variables are missing the majority of their values (i.e. NA), we have found that a number of variables are mostly empty. 

```{r checkNAs, cache=TRUE}
nafunc <- function(x) sum(is.na(x))
sum_nas <- sapply(subtrain, nafunc)  # number of rows in each col with NA
```

Since the variables with high number of NAs provide very little or no information, those variables are removed from the analysis.  Using 90% NAs as the cutoff point, we removed 100 variables.

```{r removeNAs, cache=TRUE}
trainRows <- dim(subtrain)[1]
few_nas <- !(sum_nas >= 0.9*trainRows)   # TRUE = there are fewer NAs
dim(subtrain)[2]-sum(few_nas) # variables to remove
sum(few_nas) # variables to keep
```

By visual inspection, we found some variables, such as the subject's name and timestamps, etc. would not have any significant impact on the outcome, thus those variables are also removed.

```{r moreRemoves, cache=TRUE}
names(subtrain)[1:7]
useful <- c(rep(TRUE, times=length(subtrain)))
useful[c(1,2,3,4,5,6,7)] <- FALSE 

keepvar <- useful & few_nas
usefulVars <- sum(keepvar)

subtrain <- subtrain[ ,keepvar]
subtest <- subtest[ ,keepvar]
```

This leaves us with `r usefulVars` columns in our data, including the predicted variable.

### Model Building
Since it is a classification problem to predict the exercise type, we will use [randomForest](http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#workings) algorithm. Random Forests grows many classification trees. To classify a new object from an input vector, put the input vector down each of the trees in the forest. Each tree gives a classification, and we say the tree "votes" for that class. The forest chooses the classification having the most votes (over all the trees in the forest).
 
First, we tune `randomForest` for the optimal `mtry` parameter value with respect to *Out-of-Bag* error estimate.

```{r rftune, cache=TRUE, results='hide'}
set.seed(9988)
rf_tune <- tuneRF(subtrain[,1:(length(subtrain)-1)], subtrain$classe
                 , doBest=FALSE
                 , trace=FALSE
                 , plot=FALSE
                 )
mtry_tuned <- which.min(rf_tune)
```

The *out-of-Bag* error estimate is minimized at `r mtry_tuned` variables. This estimate is used to create the random forest model.

```{r rf, cache=TRUE}
set.seed(138)
rf_fit <- randomForest(classe ~ . , data=subtrain
                       , mtry=mtry_tuned
                       , ntree=500
                       , importance=TRUE)
rf_fit

# error plot
plot(rf_fit, main="Decrease in Model Error with Number of Trees")
```

##### *Figure 1: the model error decreases while the number of trees increases*

The graph below shows the importance of variables in the fitted model.

```{r varimp, cache=TRUE}
varImpPlot(rf_fit, main = "Importance of Predictors in the Fit", 
           pch=19, col="blue",cex=0.75, sort=TRUE, type=1)
```

##### *Figure 2: variable importance plot*

### Cross-Validation
To validate the final model, we apply the model to the subset of the training data and compare the predicted classes to the actual classes.  

```{r oosError, cache=TRUE}
pred <- predict(rf_fit,newdata=subtest)
cm <- confusionMatrix(pred,subtest$classe)
oos_err <- sum(!(pred==subtest$classe))/dim(subtest)[1] 

# estimated out of sample error
oos_err
```

The confusion matrix shows that the model predicts each class with high precision.  The estimated the out-of-sample error is `r oos_err`, or `r oos_err*100`%. 

### Test Results
The model is applied to the test cases given with the 53 selected features/variables. The predictions from this test set were submitted for the assignment. The feedback upon submission indicated that the model has a 100% accuracy. 

```{r test, cache=TRUE}
testing <- testing[ ,keepvar]
pred2 <- predict(rf_fit,newdata=testing)

### submission file script from course website 
pml_write_files = function(x){
        n = length(x)
        for(i in 1:n){
                filename = paste0("problem_id_",i,".txt")
                write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
        }
}

pml_write_files(pred2)
```